Variable	Variable name	Measurement unit / data type	Allowed values	Definition	Description
normalized_representation_mse	Metamer normalized mean-squared representation error	float	Any finite float (no NaN or infinity)	Normalized representation MSE is an error metric that should be comparable across models and images	Computed by metamer.normalized_mse(). This takes the difference between the model's representation of the putative metamer and the reference image, squares it, and divides by the variance of the model's representation fo the reference image. This was the metric used to check for synthesis convergence in Freeman and Simoncelli, 2011, where it was below 0.01 for all images.
num_iterations	Number of synthesis iterations	int	Any non-negative finite integer up to 750 (for RGC metamers) or 5000 (for V1 metamers); RGC metamers should almost all reach 750, V1 will often stop before hitting the limit but should still reach 3000 or so	The number of iterations is a duration of synthesis that is measured in optimization iterations	We do metamer synthesis using iterative optimization, and we continue optimization until we either hit the maximum number of iterations (max_iter), the loss has changed by less than loss threshold (loss_thresh) or we hit a NaN (which typically comes because the gradient has gotten very small), whichever comes first. This number specifies the number of iterations that were actually undergone before synthesis ended.
loss	L2 loss	float	Any non-infinity float (NaN is possible)	The L2 loss is an error metric that is used for metamer synthesis, but is not necessarily comprable across models and images	Computed by metamer.objective_function(), this is the L2-norm of the difference between the model's representations of the putative metamer and the reference image and is used as the objective function for the optimization that takes place during metamer synthesis (so the goal is to minimize this number).
num_statistics	Number of model statistics	int	Any positive finite integer	The number of model statistics is a measure of model complexity that is a product of the number of pooling windows and the number of different statistic types generated by the model	This is simply the number of elements in the representation returned by the model, model(image).numel(). A reasonable expectation is that the more of these you have, the better the metamer will look, and so models taht use a small number of statistic types will need more pooling windows.
image_mse	Image pixelwise mean-squared error	float	Any finite float (no NaN or infinity)	Image pixelwise mean-squared error is a measure of the difference between the generated metamer and the reference image that has nothing to do with perception.	Because of how the metamers are generated, the distance between the metamers and the corresponding reference images in model representation space will be very close to zero. It's interesting to compare this to the distance in pixel space; a good / useful / interesting metamer will have a very large number here: it's very physically distinct, but still perceptually identical.
duration_human_readable	Human-readable synthesis duration	DD:HH:MM:SS. Days, hours, minutes, seconds; all except seconds are integers, seconds will be displayed to 3 decimal points	All fields are non-negative	The duration of synthesis (not including time to prepare inputs and save outputs, which can be substantial) in human-readable format	This is the human-readable version of the duration field, and is a str. Thus, no arithmetic should be carried out on it.
duration	The synthesis duration	Seconds / float	Any non-negative finite float	The duration of synthesis (not including time to prepare inputs and save outputs, which can be substantial) in seconds	This is the value that is used to create the duration_human_readable field.
optimizer	Optimization algorithm	string	Adam, SGD, LBFGS	The optimization algorithm used for metamer synthesis, chosen from among pytorch's optimizers.	In theory, this could be any of the optimizers support by pytorch, but our metamer.py script only supports these three, and of those, Adam is consistently the best, so it's the only one found in this data.
fraction_removed	Fraction of representation ignored when computing loss and gradient	float	Any float between 0 and 1 (no NaN or infinity)	fraction_removed is an optimization hyperparameter that tries to inject stochasticity into the computation of the gradient in order to generate more diverse metamers.	Every iteration of synthesis, we compute the loss between the model's representation of the metamer-in-progress and the reference image. fraction_removed allows us randomly select a subset of the representation to do this with, giving a stochastic estimate of the gradient which may help optimization and enable us to more extensively explore the space of possible metamers. In practice, this doesn't seem to have a big effect on our models, and so is always set to 0 (in which case we always compute the loss/gradient with respect to all items in the representation).
model	Model name	str	RGC_cone-1.0_gaussian, V1_cone-1.0_norm_s6_gaussian	The model name is the string which identifies which type of model to build.	See the docstring of create_metamers.main() for more details on how this string is structured, but the string identifies the visual area we're modeling, the cone non-linearity, the window type, the number of scales to include (V1 only) and whether to normalize or not (V1 only). There are many more possible variants than the two given in Allowed values, but those are the only two used in this experiment.
target_image	Path to the reference image for metamer synthesis	str	Must be a valid unix path	target_image is the full path to the reference image used for metamer synthesis.	Since this is the full path to the reference image, it will not be usable across different file systems (e.g., if you synthesize the metamers on an HPC cluster and then copy the results back to your local machine, this will not be the location fo the reference image).
seed	Random seed	int	Any non-negative finite integer	The random seed is used to set the state fo the random number generators for both numpy and pytorch in order to increase replicability.	As far as I'm aware, there's no systematic relationship between the RNG state at subsequent values, so we just use seeds 0, 1, and 2 for ease of use. Note that pytorch does not guarantee replicability between CPU and GPU (even with same seeds).
learning_rate	Optimization learning rate	float	Any positive finite float (no NaN or infinity)	The learning rate is an optimization hyperparamter that determines how big the steps during optimization should be.	There are many possible values that can be taken for this. Too small and optimization will take too long, too large and the optimization will not find any good solutions. 0.1 seems to work well for both models used. Note that we use a learning rate scheduler in Metamer.synthesize(), so this learning rate just serves as the initial learning rate; it will decrease over time.
loss_change_thresh	Loss change threshold	float	Any non-negative finite float (no NaN or infinity), should be relatively small	loss_change_thresh is an optimization hyperparameter that determines when we consider the loss to have stopped changing for the purposes of some optimization tricks we use.	If the loss changes less than loss_change_thresh between two iterations, we consider that the loss has stopped changing for the purposes of coarse-to-fine optimization and loss_change_fraction. It should thus be strictly larger than loss_thresh (or 0), in order to avoid complications where both thresholds are passed at once.
coarse_to_fine	Coarse-to-fine optimization	bool	True or False	Coarse-to-fine optimization is a way to do metamer synthesis with models (like our V1 models) that have representations at multiple scales.	This is an argument passed to metamer.synthesize(). If True, we optimize from the coarsest scales to the finest (starting with mean pixel intensity, then moving to the "top" of the steerable pyramid and progressing downwards; the previous scales are left free to drift and are not clamped in any way), and then eventually optimize with respect to all of them at once. It greatly helps with finding a good solution, though the resulting videos are weird to watch. Therefore, this is always True the V1 metamers (it's always false for RGC, because that model doesn't have multiple scales). When True, we move on from one scale when the loss has changed by less than loss_change_thresh in the past iteration.
loss_change_fraction	Loss change fraction	float	Any non-negative finite float (no NaN or infinity)	loss_change_fraction is an optimization hyperparamter that determines what fraction of the representation to use to compute the loss (and therefore gradient) if it appears that the loss has stopped decreasing.	If you think the optimization is regularly getting stuck in local optima, this is one way to try and jump out of them: when loss changes by less than loss_change_thresh, only compute the gradient with respect to a subset of the representation; hopefully this will pop you out of the optima and allow you to continue on your way. In practice, this didn't seem to work very well (and wasn't necessary; coarse-to-fine optimization turne dout to be a more effective way of dealing with local optima), so it's always set to 1 (in which case you always calculate gradient with respect to all of the representation).
initial_image	Type of image used to initialize metamer synthesis	str	white, blue, pink, gray, or path to a file	initial_image determines the image used to initialize metamer synthesis.	initial_image is typically a type of noise (white, blur, or pink). For the experiment, we only used white noise; the other options are useful for trying to better understand what's happening during synthesis.
num_gpus	Number of GPUs used during metamer synthesis	int	0 through 8, inclusive	num_gpus is the number of GPUs used during metamer synthesis.	Using GPUs with pytorch greatly increases the speed of all the basic operations, and so greatly speeds up metamer synthesis (the main determinant of how long it will take is how long a single forward call takes). The fastest is with a single GPU; when using multiple GPUs, time is lost copying things back and forth between them and so it's not clear if it even represents a large advantage over CPU-only (especially because GPUs are a limited resource). Pytorch cannot guarantee reproducibility between CPUs and GPUs, so a CPU-only metamer might not be identical to one using the GPUs, even if everything (including the random seed) is identical.
min_ecc	Model's minimum eccentricity	float	Minimum value of 0.5, must be strictly less than max_ecc (and hopefully a good deal less, otherwise it's not an interesting metamer)	min_ecc is the eccentricity at which the model starts constructing its pooling windows; the model is blind to pixels below this eccentricity and thus the metamer will match them pixel-for-pixel.	There are two justifications for this: physiologically, it's reasonable to think of a floor on receptive field size, the smallest one could possibly be is a single photoreceptor. But more important, technically, we're limited by the pixellation of the image: we can't match statistics in a region smaller than pixel. This depends on the scaling value: the smaller the scaling value, the larger the min_ecc. However, for our experiment, all metamers from a given model have the same min_ecc, based on the smallest scaling value in the experiment (in order to make performance comparisons across scaling conditions easier).
max_ecc	Model's maximum eccentricity	float	A positive finite float, strictly greater than min_ecc (and hopefully a good deal larger, otherwise it's not an interesting metamer)	max_ecc is the eccentricity of farthest edge of the image, measured from the center.	This is based on the experimental setup and should be the extent that the image will fill when displayed on your experimental setup. This is thus constant for all metamers in the experiment, regardless of model. The goal of this project was to extend them as far into the periphery as possible, so this is a reasonably large number, 41. If an image is non-square (like ours), this should be the extent of the larger dimension; the model will figure things out correctly.
max_iter	Maximum number of iterations during metamer synthesis	int	Any positive finite integer (no NaN or infinity)	max_iter is the maximum number of iterations we'll do in metamer synthesis	We do metamer synthesis using iterative optimization, and we continue optimization until we either hit the maximum number of iterations (max_iter), the loss has changed by less than loss threshold (loss_thresh) or we hit a NaN (which typically comes because the gradient has gotten very small), whichever comes first. This number is the same for all metamers generated by a given model: 750 for RGC and 5000 for V1, which (because of the larger number of statistics and because it's non-linear) takes longer to find a good solution.
loss_thresh	Loss threshold for ending synthesis	float	Any positive finite float (no NaN or infinity), should be very small and strictly less than loss_change_thresh (assuming loss_change_thresh is non-zero).	loss_thresh is the threshold we use to determine whether the loss has stopped decreasing. 	We do metamer synthesis using iterative optimization, and we continue optimization until we either hit the maximum number of iterations (max_iter), the loss has changed by less than loss threshold on consecutive iterations (loss_thresh) or we hit a NaN (which typically comes because the gradient has gotten very small), whichever comes first. This number is the same for all metamers, 1e-8. In practice, we tend to hit one of the other termination conditions first (either hitting a NaN or max_iter iterations).
scaling	Pooling window scaling value	float	Any positive finite float (no NaN or infinity)	scaling gives the slope of the linear relationship between pooling window full-width half-maximum and eccentricity	This is the model parameter that we're testing in our experiment, and we test 9 values for each model. It's inspired by the established linear relationship between neural receptive fields and eccentricity. As successive areas in the ventral stream have larger and larger scaling values, so too do the values we test: RGC has the smallest set of values, then V1, and then V2, but each successive model will have at least one scaling value that overlaps, for ease of comparison.
clamper	Type of clamping done during synthesis iteration	str	clamp, clamp2, clamp4, remap	clamper is a str that gives what type of clamping we do during synthesis iteration	The optimization algorithm is unconstrained and so can set any pixel values; clamping forces the image to match some constraints. remap remaps the full range, adding the minimum and dividing by the maximum so everything lies between 0 and 1; clamp sets the minimum to 0 and the maximum to 1 (setting all negative values to 0 and all values greater than 1 to 1, without affecting any other values); clamp2 additionally clamps the mean and standard deviation to that of the reference image; clamp4 additionally sets the skew and kurtosis to that of the initial image. Of these, clamp is the most "neutral" option, clamp2 seems to help synthesis the most (producing metamers with the lowest loss), remap is not recommended because it results in weird images, and clamp4 does not consistently work at this time (occasionally hits errors that have been hard to debug). We always use clamp2 in our experiment.
clamp_each_iter	Whether we run the clamper each iteration or not	bool	True, False	clamp_each_iter is an optimization hyperparameter that determines whether we apply the clamper each iteration (True) or only at the end of synthesis (False).	This is always True in our experiment, and is recommended to always be set like that, because otherwise the image you get out at the end of synthesis will be different from the one that was optimized.
clip_grad_norm	Whether we clip the gradient norm or not	bool or float	True, False, or positive finite float (no NaN or infinity)	clip_grad_norm is an optimization hyperparameter that determines whether we call torch.nn.utils.clip_grad_norm each iteration in order to try and prevent the gradient from growing too large.	During metamer synthesis, you can have trouble finding good solutions if the gradient grows out of control, and clipping the gradient can help with that. If a float, this is the max gradient norm we allow, and if True, we use a value of 1, which seems to work well. This seems to help with a non-convex cone_power value, but not enough: in our experiment, we set this to False, used cone_power=1, and pre-processed our reference images to raise every pixel to the 1/3 before synthesis.
image_name	Name of the reference image	str	azulejos_cone_full_size-2048,3528; market_cone_full_size-2048,3528; tiles_cone_full_size-2048,3528; flower_cone_full_size-2048,3528	image_name is the name of the reference, which involves taking the file name from target_image and removing the extension.	Technically, many more values are possible than those given in Allowed values, but these are the four from of our experiment.
image_name_for_expt	Name of the reference image for comparison in experiment	str	azulejos_full_size-2048,3528; market_full_size-2048,3528; tiles_full_size-2048,3528; flower_full_size-2048,3528	image_name_for_expt is the name of the reference image we will compare with this metamer in the experiment.	This is the same as image_name, except witH "cone_" removed, if that str was present. This is because we pre-processed the image for synthesis, but want to show the linear version of the reference image in the experiment.